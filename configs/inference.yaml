
model:
  target: unifuture.models.diffusion.DiffusionEngine
  params:
    input_key: img_seq
    scale_factor: 0.18215
    disable_first_stage_autocast: True
    en_and_decode_n_samples_a_time: 1
    num_frames: &num_frames 25

    ################## perception #########################
    perception_mem_save_mode: False
    perception_mode: align_latent_multiscale
    perception_feedback: True   # outside feedback, depth latent will add back to diffusion latent
    train_perception_only: False  # also train the UNet

    depth_target_engine_config:
      target: unifuture.modules.perception.depth.DepthTargetEngine
      params: 
        ckpt_path: ckpts/depth_anything_v2_vitl.pth 
        engine_config:
          target: unifuture.modules.perception.depth_anything_v2.dpt.DepthAnythingV2
          params:
            encoder: vitl
            features: 256
            out_channels: [256, 512, 1024, 1024]
            use_bn: False
            use_clstoken: False
        depth_mapping_config:
          target: unifuture.modules.perception.depth_utils.MinMaxDepthTargetMapping
          params:
            near_plane: -1.0
            far_plane: 1.0
    ################## perception #########################

    denoiser_config:
      target: unifuture.modules.diffusionmodules.denoiser.Denoiser
      params:
        num_frames: *num_frames

        scaling_config:
          target: unifuture.modules.diffusionmodules.denoiser_scaling.VScalingWithEDMcNoise

    network_config:
      target: unifuture.modules.perception.depth.ParallelVideoUNetWithDepthBranch
      params:
        adm_in_channels: 768
        num_classes: sequential
        use_checkpoint: False
        in_channels: 8
        out_channels: 4
        model_channels: 320
        attention_resolutions: [ 4, 2, 1 ]
        num_res_blocks: 2
        channel_mult: [ 1, 2, 4, 4 ]
        num_head_channels: 64
        use_linear_in_transformer: True
        transformer_depth: 1
        context_dim: 1024
        spatial_transformer_attn_type: softmax-xformers
        extra_ff_mix_layer: True
        use_spatial_context: True
        merge_strategy: learned_with_images
        video_kernel_size: [ 3, 1, 1 ]
        add_lora: False
        action_control: True
        ################## perception #########################
        perception_detach_mode: False  # train with UNet jointly
        perception_mode: align_latent_multiscale
        perception_feedback: True   # inside feedback, depth latent will add back to diffusion latent for each scale
        depth_branch_config:
          target: unifuture.modules.perception.depth.MultiScaleDepthAlignMlp
          params:
            out_channels: 4
            num_features: 320  # the same as VideoUNet.model_channels
            num_scale: 4
            num_block_each_stage: 2
            hidden_kernel: [1, 3, 5, 7]
            channel_mult: [4, 4, 2, 1]
            multiscale_positional_encoding:
              target: unifuture.modules.perception.depth_utils.MultiscalePositionalEmbeddingWrapper
              params:
                base_target: unifuture.modules.perception.depth_utils.LearnablePositionalEmbedding
                base_num_features: 320
                base_grid_height: 40  # 320 / 8
                base_grid_width: 72  # 576 / 8
                channel_mults: [4, 4, 2, 1]
                down_scales: [8, 4, 2, 1]
        ################## perception #########################

    conditioner_config:
      target: unifuture.modules.GeneralConditioner
      params:
        emb_models:
          - input_key: cond_frames_without_noise
            is_trainable: False
            target: unifuture.modules.encoders.modules.FrozenOpenCLIPImagePredictionEmbedder
            params:
              n_cond_frames: 1
              n_copies: 1
              open_clip_embedding_config:
                target: unifuture.modules.encoders.modules.FrozenOpenCLIPImageEmbedder
                params:
                  freeze: True

          - input_key: fps_id
            is_trainable: False
            target: unifuture.modules.encoders.modules.ConcatTimestepEmbedderND
            params:
              outdim: 256

          - input_key: motion_bucket_id
            is_trainable: False
            target: unifuture.modules.encoders.modules.ConcatTimestepEmbedderND
            params:
              outdim: 256

          - input_key: cond_frames
            is_trainable: False
            target: unifuture.modules.encoders.modules.VideoPredictionEmbedderWithEncoder
            params:
              disable_encoder_autocast: True
              n_cond_frames: 1
              n_copies: 1
              is_ae: True

              encoder_config:
                target: unifuture.models.autoencoder.AutoencoderKLModeOnly
                params:
                  embed_dim: 4
                  monitor: val/rec_loss

                  ddconfig:
                    attn_type: vanilla-xformers
                    double_z: True
                    z_channels: 4
                    resolution: 256
                    in_channels: 3
                    out_ch: 3
                    ch: 128
                    ch_mult: [ 1, 2, 4, 4 ]
                    num_res_blocks: 2
                    attn_resolutions: [ ]
                    dropout: 0.0

                  loss_config:
                    target: torch.nn.Identity

          - input_key: cond_aug
            is_trainable: False
            target: unifuture.modules.encoders.modules.ConcatTimestepEmbedderND
            params:
              outdim: 256

          - input_key: command
            is_trainable: False
            target: unifuture.modules.encoders.modules.ConcatTimestepEmbedderND
            params:
              outdim: &action_emb_dim 128
              num_features: 1
              add_sequence_dim: True

          - input_key: trajectory
            is_trainable: False
            target: unifuture.modules.encoders.modules.ConcatTimestepEmbedderND
            params:
              outdim: *action_emb_dim
              num_features: 8
              add_sequence_dim: True

          - input_key: speed
            is_trainable: False
            target: unifuture.modules.encoders.modules.ConcatTimestepEmbedderND
            params:
              outdim: *action_emb_dim
              num_features: 4
              add_sequence_dim: True

          - input_key: angle
            is_trainable: False
            target: unifuture.modules.encoders.modules.ConcatTimestepEmbedderND
            params:
              outdim: *action_emb_dim
              num_features: 4
              add_sequence_dim: True

          - input_key: goal
            is_trainable: False
            target: unifuture.modules.encoders.modules.ConcatTimestepEmbedderND
            params:
              outdim: *action_emb_dim
              num_features: 2
              add_sequence_dim: True

    first_stage_config:
      target: unifuture.models.autoencoder.AutoencodingEngine
      params:
        loss_config:
          target: torch.nn.Identity

        regularizer_config:
          target: unifuture.modules.autoencoding.regularizers.DiagonalGaussianRegularizer

        encoder_config:
          target: unifuture.modules.diffusionmodules.model.Encoder
          params:
            attn_type: vanilla
            double_z: True
            z_channels: 4
            resolution: 256
            in_channels: 3
            out_ch: 3
            ch: 128
            ch_mult: [ 1, 2, 4, 4 ]
            num_res_blocks: 2
            attn_resolutions: [ ]
            dropout: 0.0

        decoder_config:
          target: unifuture.modules.autoencoding.temporal_ae.VideoDecoder
          params:
            attn_type: vanilla
            double_z: True
            z_channels: 4
            resolution: 256
            in_channels: 3
            out_ch: 3
            ch: 128
            ch_mult: [ 1, 2, 4, 4 ]
            num_res_blocks: 2
            attn_resolutions: [ ]
            dropout: 0.0
            video_kernel_size: [ 3, 1, 1 ]